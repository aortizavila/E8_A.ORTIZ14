Los métodos de ensamble ayudan a mejorar los resultados de machine learning. Los métodos de ensamble se pueden categorizar de varias formas, pero la más comunes son dos: una es usar clasificadores similares y combinarlos usando técnicas como bagging, boosting o random forests; la segunda es combinar diferentes clasificadores usando model stacking. Los métodos de ensamble son algoritmos que combinan varias técnicas de machine learning en un único modelo predictivo en orden a disminuir la variancia (bagging), bias (boosting), o la mejora de la de las predicciones. Otra división de los métodos de ensambles es: Métodos de ensamble secuencia y métodos de ensamble paralelos. La principal motivación de los métodos de ensamble de secuencia es explorar la dependencia entre las bases de aprendizaje. Para los modelos de ensamble paralelo el objetivo es explorar la independencia entre la base de aprendizaje con el objetivo de minimizar el error. Los modelos más usados tienen como base que el algoritmo de aprendizaje produce base de aprendizajes homogéneos. A este tipo de aprendizaje se le conoce como ensambles homogéneos. Hay otros modelos que se basan en aprendizajes heterogéneos conocidos como ensambles heterogéneos. Entro los métodos de ensamble tenemos: baggin, boosting. La técnica de bagging consiste en crear diferentes modelos con muestras aleatorias con reemplazo y luego ensamblar los resultados. Es un modelo de clasificación de aprendizaje supervisado. El método boosting combina los resultados de varios clasificadores débiles para obtener uno más robusto. En un clasificador débil muchas veces ganan peso lo casos que son mal clasificados y los que son clasificados correctamente pierden peso. Stacking es una forma de ensamblar múltiples clasificaciones o modelos de regresión. Con stacking se busca atacar un problema de aprendizaje con diferentes tipos de modelos, los cuales están en la capacidad de aprender alguna parte del problema, pero no siempre la totalidad del mismo.